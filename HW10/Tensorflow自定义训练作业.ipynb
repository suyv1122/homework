{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 简答题",
   "id": "900a6e19bb0fa847"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. TensorFlow 是否可以简单替代 NumPy？两者之间的主要区别是什么？\n",
    "- 可以，numpy多數函數都有對應的tensorflow函數，兩者間的主要區別在於數據類型與定義方式不同，但在複雜編程中則需要慎重考慮，因為兩者有不同的運行規則及適用場景；\n",
    "- Numpy多沿用python原本數據結構做為底層邏輯(多維數組)，而TF使用了張量與變量作為基礎數據\n",
    "\n",
    "\n",
    "2. 使用 `tf.range(10)` 和 `tf.constant(np.arange(10))` 是否会得到相同的结果？\n",
    "- 不相同，tf.range默认创建的数据类型为32位浮点数，而np.arange(10)使用64位浮点数\n",
    "\n",
    "\n",
    "3. 可以通过编写函数或继承 `tf.keras.losses.Loss` 来定义自定义损失函数。两种方法分别应该在什么时候使用？\n",
    "- 继承自tf.keras.losses.Loss的自定义类式损失函数被用于随着模型保存与恢复超参数，如果没有这种需求可以通过函数式实现，这样更简单\n",
    "\n",
    "\n",
    "4. 可以直接在函数中定义自定义指标或采用 `tf.keras.metrics.Metric` 子类。两种方法分别应该在什么时候使用？\n",
    "- 针对于简单指标可以写成函数式(简易)，针对复杂指标则需要超类继承的自定义类式\n",
    "\n",
    "\n",
    "5. 什么时候应该自定义层而不是自定义模型？\n",
    "- 当tf.keras内置的层无法满足需求时，或是网络架构中存在许多重复结构为了更清晰的编程表达，可以考虑自定义层\n",
    "\n",
    "\n",
    "6. 有哪些示例需要编写自定义训练循环？\n",
    "- 典型例子有宽深神经网络论文中展示的内容：因为其宽路径与深路径各使用了不同的优化器，需要自行编写自定义训练循环以支持这一点\n",
    "\n",
    "\n",
    "7. 自定义 Keras 组件中可以包含任意 Python 代码，还是必须转换为 TF 函数？\n",
    "- 尽量转换为TF函数以确保兼容和可移植性，像是通过@tf.function修饰器等，以避免不可预见的问题破坏可移植性\n",
    "\n",
    "8. 如果要将函数转换为 TF 函数，应避免哪些主要模式？\n",
    "- 避免使用原生循环，不同的Py函数必须遵守相同规则，随机数生成必须使用TF模块提供的方法，并需要注意带有额外执行的py函数可能不会每一次都执行\n",
    "\n",
    "9. 何时需要创建动态 Keras 模型？ 如何动态创建Keras模型？为什么不是所有模型都动态化？\n",
    "- 模型结构于前向传播中依赖输入数据，或需要灵活控制执行流程时会需要动态模型\n",
    "- 通过继承tf.keras.Model的自定义类并重写call()方法实现\n",
    "- 相比静态模型训练效率较低、更难达成保存与加载等序列操作，且无法像计算图那般进行全面优化\n"
   ],
   "id": "f0568883493d9ef7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T14:15:49.458694Z",
     "start_time": "2025-09-14T14:15:49.451697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.activations import softmax\n",
    "\n",
    "tf.constant(np.arange(10)) # <tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])>\n",
    "tf.range(10) # <tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
   ],
   "id": "bbb3725905c8922a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 编程题",
   "id": "9a282b6d9adda052"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1. 实现一个执行层归一化的自定义层：\n",
    "    - a. `build()` 方法应定义两个可训练的权重 α 和 β，它们的形状均为 `input_shape[-1:]`，数据类型为 `tf.float32`。α 应该用 1 初始化，而 β 必须用 0 初始化。\n",
    "    - b. `call()` 方法应计算每个实例特征的均值和标准差。为此，可以使用 `tf.nn.moments(inputs, axes=-1, keepdims=True)`，它返回同一实例的均值 μ 和方差 σ²（计算方差的平方根便可获得标准差）。然后，该函数应计算并返回\n",
    "      $$\n",
    "      \\alpha \\otimes \\frac{(X-\\mu)}{(\\sigma+\\epsilon)} + \\beta\n",
    "      $$\n",
    "      其中 ε 是表示项精度的一个常量（避免被零除的小常数，例如 0.001）,$\\otimes$表示逐个元素相乘\n",
    "    - c. 确保自定义层产生与tf.keras.layers.LayerNormalization层相同（或几乎相同）的输出。\n",
    "\n",
    "2. 使用自定义训练循环训练模型来处理Fashion MNIST数据集（13_神经网络介绍 里用的数据集）：\n",
    "\n",
    "    - a.显示每个轮次、迭代、平均训练损失和每个轮次的平均精度（在每次迭代中更新），以及每个轮次结束时的验证损失和精度。\n",
    "    - b.尝试对上面的层和下面的层使用具有不同学习率的不同优化器。"
   ],
   "id": "5cd3d7096f87afd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:14:40.615263Z",
     "start_time": "2025-09-14T15:14:40.596736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 实现一个执行层归一化的自定义层：\n",
    "#     - a. `build()` 方法应定义两个可训练的权重 α 和 β，它们的形状均为 `input_shape[-1:]`，数据类型为 `tf.float32`。α 应该用 1 初始化，而 β 必须用 0 初始化。\n",
    "#     - b. `call()` 方法应计算每个实例特征的均值和标准差。为此，可以使用 `tf.nn.moments(inputs, axes=-1, keepdims=True)`，它返回同一实例的均值 μ 和方差 σ²（计算方差的平方根便可获得标准差）。然后，该函数应计算并返回\n",
    "#       $$\n",
    "#       \\alpha \\otimes \\frac{(X-\\mu)}{(\\sigma+\\epsilon)} + \\beta\n",
    "#       $$\n",
    "#       其中 ε 是表示项精度的一个常量（避免被零除的小常数，例如 0.001）,$\\otimes$表示逐个元素相乘\n",
    "#     - c. 确保自定义层产生与tf.keras.layers.LayerNormalization层相同（或几乎相同）的输出。\n",
    "\n",
    "import tensorflow as tf\n",
    "class MyLayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, axis=-1, epsilon=1e-3, **kwargs):\n",
    "        # axis: 要归一化的轴（默认为 -1，即最后一个轴）\n",
    "        # epsilon: 避免除以 0 的小常数（题目示例用了 0.001）\n",
    "        super().__init__(**kwargs)\n",
    "        if isinstance(axis, int):\n",
    "            self.axis = (axis,)\n",
    "        else:\n",
    "            self.axis = tuple(axis)\n",
    "        self.epsilon = float(epsilon)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape 是一个序列（例如 (batch, ..., dim)）\n",
    "        input_shape = tf.TensorShape(input_shape).as_list()\n",
    "        # 取最后一维大小作为参数的形状\n",
    "        dim = int(input_shape[-1])\n",
    "        # α 初始化为 1，β 初始化为 0；dtype 要求题目是 tf.float32\n",
    "        self.alpha = self.add_weight(\n",
    "            name=\"alpha\",\n",
    "            shape=(dim,),\n",
    "            initializer=tf.keras.initializers.Ones(),\n",
    "            dtype=tf.float32,\n",
    "            trainable=True\n",
    "        )\n",
    "        self.beta = self.add_weight(\n",
    "            name=\"beta\",\n",
    "            shape=(dim,),\n",
    "            initializer=tf.keras.initializers.Zeros(),\n",
    "            dtype=tf.float32,\n",
    "            trainable=True\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 1) 计算按最后一维（features）每个实例的均值和方差\n",
    "        #    keepdims=True 保持最后一维为 1，便于广播\n",
    "        mean, var = tf.nn.moments(inputs, axes=self.axis, keepdims=True)\n",
    "        std = tf.sqrt(var)\n",
    "\n",
    "        # 2) 标准化： (X - μ) / (σ + ε)\n",
    "        normalized = (inputs - mean) / (std + self.epsilon)\n",
    "\n",
    "        # 3) 把 alpha/beta 转为和 inputs 相同的数据类型再广播\n",
    "        alpha = tf.cast(self.alpha, inputs.dtype)\n",
    "        beta = tf.cast(self.beta, inputs.dtype)\n",
    "\n",
    "        # alpha 的形状是 (features,) —— 这会自动广播到 (..., features)\n",
    "        return alpha * normalized + beta\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\"axis\": self.axis, \"epsilon\": self.epsilon})\n",
    "        return cfg\n"
   ],
   "id": "4cb9da9f5e9ea249",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.normalization.Normalization at 0x121edffd0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T15:24:05.744596Z",
     "start_time": "2025-09-14T15:24:05.197792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. 使用自定义训练循环训练模型来处理Fashion MNIST数据集（13_神经网络介绍 里用的数据集）：\n",
    "#\n",
    "#     - a.显示每个轮次、迭代、平均训练损失和每个轮次的平均精度（在每次迭代中更新），以及每个轮次结束时的验证损失和精度。\n",
    "#     - b.尝试对上面的层和下面的层使用具有不同学习率的不同优化器。\n",
    "import tensorflow as tf\n",
    "\n",
    "# 加载Fashion MNIST。它已经被打乱并分成了训练集（60000个图像）和测试集（10000个图像）\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test , y_test) = fashion_mnist\n",
    "\n",
    "# 保留训练集中的最后5000个图像进行验证\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n"
   ],
   "id": "f5ca4bac248e5dfb",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ========== 模型 ==========\n",
    "class WideDeepModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.wide = tf.keras.layers.Dense(1)   # wide 部分\n",
    "        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.hidden2 = tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "        self.out = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        wide_out = self.wide(inputs)\n",
    "        deep_out = self.hidden1(inputs)\n",
    "        deep_out = self.hidden2(deep_out)\n",
    "        return self.out(wide_out + deep_out)   # wide + deep\n",
    "\n",
    "model = WideDeepModel()\n",
    "\n",
    "# ========== 优化器 ==========\n",
    "wide_optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "deep_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_mae = tf.keras.metrics.MeanAbsoluteError(name=\"train_mae\")\n",
    "val_loss = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "val_mae = tf.keras.metrics.MeanAbsoluteError(name=\"val_mae\")\n",
    "\n",
    "# ========== 训练循环 ==========\n",
    "def random_batch(X, y, batch_size=32):\n",
    "    idx = np.random.randint(len(X), size=batch_size)\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 32\n",
    "n_steps = len(X_train) // batch_size\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{n_epochs}\")\n",
    "    for step in range(1, n_steps + 1):\n",
    "        X_batch, y_batch = random_batch(X_train_scaled, y_train, batch_size)\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred = model(X_batch, training=True)\n",
    "            loss_value = loss_fn(y_batch, y_pred)\n",
    "\n",
    "        # 分别获取 wide 和 deep 参数\n",
    "        wide_vars = model.wide.trainable_variables\n",
    "        deep_vars = model.hidden1.trainable_variables + \\\n",
    "                    model.hidden2.trainable_variables + \\\n",
    "                    model.out.trainable_variables\n",
    "\n",
    "        wide_grads = tape.gradient(loss_value, wide_vars)\n",
    "        deep_grads = tape.gradient(loss_value, deep_vars)\n",
    "\n",
    "        wide_optimizer.apply_gradients(zip(wide_grads, wide_vars))\n",
    "        deep_optimizer.apply_gradients(zip(deep_grads, deep_vars))\n",
    "\n",
    "        train_loss(loss_value)\n",
    "        train_mae(y_batch, y_pred)\n",
    "\n",
    "        # 进度条\n",
    "        if step % 100 == 0 or step == n_steps:\n",
    "            print(f\"\\rStep {step}/{n_steps} - \"\n",
    "                  f\"loss: {train_loss.result():.4f} - \"\n",
    "                  f\"mae: {train_mae.result():.4f}\", end=\"\")\n",
    "\n",
    "    # 验证集评估\n",
    "    y_val_pred = model(X_valid_scaled, training=False)\n",
    "    val_loss(loss_fn(y_valid, y_val_pred))\n",
    "    val_mae(y_valid, y_val_pred)\n",
    "\n",
    "    print(f\" - val_loss: {val_loss.result():.4f} - val_mae: {val_mae.result():.4f}\")\n",
    "\n",
    "    # reset metrics\n",
    "    train_loss.reset_states()\n",
    "    train_mae.reset_states()\n",
    "    val_loss.reset_states()\n",
    "    val_mae.reset_states()\n"
   ],
   "id": "eba6557271d47f4f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
