{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "简答题：\n",
    "1. 如果你的训练集具有数百万个特征，那么可以使用哪种线性回归训练算法？\n",
    "    - 随机梯度下降适配于此类数据量极大的内容，可以结合正则化来优化特征，像是弹性网络回归以及套索回归\n",
    "\n",
    "2. 如果你的训练集里特征的数值大小迥异，那么哪些算法可能会受到影响？受影响程度如何？你应该怎么做？\n",
    "    - 所有对特征尺度敏感算法都会受到影响，例如标准线性回归以及分类回归、KNN、支持向量机等\n",
    "    - 而处理这些问题优先考虑对数据做特征工程，像是统一特征尺度(归一化)，手动去除或是增加一些无关/有关特征等\n",
    "\n",
    "3. 训练逻辑回归模型时，梯度下降可能会卡在局部最小值中吗？\n",
    "    - 不会，逻辑回归模型的损失函数是凸函数，总是会收敛到全局最小值附近\n",
    "\n",
    "4. 如果你让它们运行足够长的时间，是否所有的梯度下降算法都能得出相同的模型？\n",
    "    - 不一定，就像批量梯度下降理想状况下会在接近全局最小值的位置越来越收敛——但它可能永远够不到\n",
    "    - 而随机梯度下降则更是只能到达全局最小值附近的随机位置，因此终归模型的权重之间会有或明显或细微的差别\n",
    "\n",
    "5. 假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，那么可能是什么情况？你该如何解决？\n",
    "    - 两种可能，要么学习率过高它已经在向上爬坡了——这时应当尝试调低学习率\n",
    "    - 或是模型迭代位于一个局部最小值，而它正在尝试跳出这部分，这种情况可以放长观察时间，如果一直没有脱离代价上升\n",
    "    - 再考虑是第一种情况\n",
    "\n",
    "6. 当验证误差上升时立即停止小批量梯度下降是个好主意吗？\n",
    "    - 否，考虑局部最小值的情况，任何短时间内的代价上升都应当持续观察一段时间\n",
    "    - 并以代价的全局趋势作为是否停止的判断标准\n",
    "\n",
    "7. 哪种梯度下降算法（在我们讨论过的算法中）将最快地到达最佳解附近？哪个实际上会收敛？如何使其他的也收敛\n",
    "    - 随机梯度下降可以快速抵达最优解附近，但也只能抵达附近；批量梯度下降会在接近最小值时逐渐收敛\n",
    "    - 引入学习率随迭代次数动态衰减的机制可以帮助其他算法也做到逐渐收敛\n",
    "\n",
    "8. 假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大的差距。发生了什么？解决此问题的三种方法是什么？\n",
    "    - 考虑到使用的回归是多项式回归，这种情况下较大概率训练出了高方差模型(对训练集过拟合)\n",
    "    - 解决的三种方法...您真的很喜欢三这个数字，是受到gabe影响吗？\n",
    "    - 减少多项式阶数，加入正则化项，增加训练集数据量均可解决\n",
    "\n",
    "9. 假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高偏差或高方差？你应该增加正则化超参数α还是减小它呢？\n",
    "    - 高偏差，即模型对训练集与测试集的拟合表现都很差，欠拟合症状，考虑减小超参数alpha的值来增加异常值的权重\n",
    "    - 由此增加模型对训练集的拟合——不要根据测试集的表现去调整超参数\n",
    "\n",
    "10. 为什么要使用：a.岭回归而不是简单的线性回归（即没有任何正则化）？b.Lasso而不是岭回归？c.弹性网络而不是Lasso回归？\n",
    "    - a. 有正则化总是比没有强(?)，你可以把alpha调的很低来达成线性回归的效果，而不是在结果过拟合时再重新写一个正则化回归\n",
    "    - b. 没用的特征太多时套索回归可以替代做出特征筛选，把弱相关特征的权重降低来节约计算成本，并避免这些特征对模型的影响\n",
    "    - c. 当特征数量显著多于训练集数量，或是几个特征强相关时，套索回归的表现会变得不稳定，此时应换用弹性网络回归\n",
    "\n",
    "11. 假设你要将图片分类为室外/室内和白天/夜间。你应该实现两个逻辑回归分类器还是一个softmax回归分类器？\n",
    "    - 两个逻辑回归分类器，这两个二项标签是可以组合的，像是白天的室外以及夜晚的室内等(多标签分类)\n",
    "    - 室内？ 室外？ 白天？ 夜间？ 室内白天？ 室内夜间？ 室外白天？ 室外夜间？"
   ],
   "id": "6af774aa03256f6f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "编程题：",
   "id": "10695eaf8992f30e"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:18.943581Z",
     "start_time": "2025-08-10T17:20:18.939771Z"
    }
   },
   "source": [
    "# todo 编程题: 在不使用sklearn的情况下，仅使用Numpy，为softmax回归实现带早停的批量梯度下降，将它用于分类任务，\n",
    "#  例如鸢尾花数据集  load_iris, 只用两个特征就可以：\"petal width (cm)\", \"petal length (cm)\"\n",
    "#  强调：除了读数据，其他全用numpy （包括分离测试+验证），不用sklearn\n",
    "\n",
    "#  注意：\n",
    "#  1. 要实现l2正则化\n",
    "#  2. 除了数据读取，其他仅使用numpy，包括训练集+验证集分离，以及softmax预测 和 损失计算"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:18.961429Z",
     "start_time": "2025-08-10T17:20:18.958828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "#\n",
    "# def softmax(logits):\n",
    "#     exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "#     probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "#     return probs\n",
    "#\n",
    "# logits = np.array([[1, 2, 3],\n",
    "#                    [1, 2, -1]])\n",
    "# print(softmax(logits))\n",
    "# print(np.sum(softmax(logits)[1,:]))\n",
    "#\n",
    "# def cross_entropy_loss(y_tr_onehot, y_pred):\n",
    "#     eps = 1e-13\n",
    "#     probs_clipped = np.cilp(y_tr_onehot, eps, 1 - eps)\n",
    "#     ce_loss = -np.sum(y_tr_onehot * np.log(probs_clipped)) / y_tr_onehot.shape[0]\n",
    "#     reg = (lambda_ / (y_tr_onehot.shape[0] * 2)) *np.sum(W * W)\n",
    "#     loss = ce_loss + reg\n",
    "#     return loss\n",
    "#\n",
    "# err = probs - y_tr_onehot\n",
    "# dW = (X_tr.T @ err) / X_tr.shape[0] + (lambda_ / X_tr.shape[0]) * W\n",
    "# db = err.sum(axis=0) /X_tr.shape[0]\n",
    "#\n",
    "# W = lambda_ * dW\n",
    "# b = lambda_ * db"
   ],
   "id": "e570eec2e1cc8291",
   "outputs": [],
   "execution_count": 121
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:18.977219Z",
     "start_time": "2025-08-10T17:20:18.974297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def onehot_change(y):\n",
    "    y = y.reshape(-1).astype(int)\n",
    "    num_classes = np.max(y) + 1\n",
    "    y_onehot = np.eye(num_classes)[y]\n",
    "    return y_onehot"
   ],
   "id": "bd4a58d345475d65",
   "outputs": [],
   "execution_count": 122
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:18.986244Z",
     "start_time": "2025-08-10T17:20:18.981540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def stratified_sampling(X,y):\n",
    "    X_train_list, y_train_list = [], []\n",
    "    X_test_list, y_test_list = [], []\n",
    "\n",
    "    for label in np.unique(y):\n",
    "        idx = np.where(y == label)[0]\n",
    "        np.random.shuffle(idx)\n",
    "        split = int(0.8 * len(idx))\n",
    "        train_idx, test_idx = idx[:split], idx[split:]\n",
    "\n",
    "        X_train_list.append(X[train_idx])\n",
    "        y_train_list.append(y[train_idx])\n",
    "        X_test_list.append(X[test_idx])\n",
    "        y_test_list.append(y[test_idx])\n",
    "\n",
    "    X_train = np.vstack(X_train_list)\n",
    "    y_train = np.concatenate(y_train_list)\n",
    "    X_test = np.vstack(X_test_list)\n",
    "    y_test = np.concatenate(y_test_list)\n",
    "    return X_train, y_train, X_test, y_test"
   ],
   "id": "759cf12c0564532",
   "outputs": [],
   "execution_count": 123
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:18.995711Z",
     "start_time": "2025-08-10T17:20:18.990889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def testmodel(X_tr, y_tr_onehot, lr, lambda_ = 0.01, patience = 5, epochs = 1000):\n",
    "    W = np.random.randn(X_tr.shape[1], y_tr_onehot.shape[1]) * 0.01\n",
    "    b = np.zeros(y_tr_onehot.shape[1])\n",
    "    eps = 1e-13\n",
    "    less_loss = None\n",
    "    best_W = None\n",
    "    best_b = None\n",
    "    patience_counter = 0\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        logits = X_tr @ W + b\n",
    "        exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "        probs_clipped = np.clip(probs, eps, 1 - eps)\n",
    "        ce_loss = -np.sum(y_tr_onehot * np.log(probs_clipped)) / y_tr_onehot.shape[0]\n",
    "        reg = (lambda_ / 2) * np.sum(W * W)\n",
    "        loss = ce_loss + reg\n",
    "\n",
    "        err = probs - y_tr_onehot\n",
    "        dW = (X_tr.T @ err) / X_tr.shape[0] + lambda_ * W\n",
    "        db = err.sum(axis=0) / X_tr.shape[0]\n",
    "        W -= lr * dW\n",
    "        b -= lr * db\n",
    "\n",
    "        if less_loss is None or loss < less_loss:\n",
    "            less_loss = loss\n",
    "            best_W = W.copy()\n",
    "            best_b = b.copy()\n",
    "            patience_counter = 0\n",
    "        else :\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"early stopping————patience counter has been reached than patience\")\n",
    "                break\n",
    "\n",
    "    return best_W, best_b"
   ],
   "id": "8ba4924be54c49fc",
   "outputs": [],
   "execution_count": 124
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:19.001729Z",
     "start_time": "2025-08-10T17:20:18.999049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(X, W, b):\n",
    "    logits = X @ W + b\n",
    "    exp_scores = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return np.argmax(probs, axis=1)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)"
   ],
   "id": "938b8182ef5bb6c4",
   "outputs": [],
   "execution_count": 125
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:19.008702Z",
     "start_time": "2025-08-10T17:20:19.004905Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X_ori = load_iris()"
   ],
   "id": "973fffb997728cfd",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:19.167097Z",
     "start_time": "2025-08-10T17:20:19.012359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = X_ori.data\n",
    "y = X_ori.target\n",
    "X_train, y_train, X_test, y_test = stratified_sampling(X, y)\n",
    "y_tr_onehot = onehot_change(y_train)\n",
    "best_W, best_b = testmodel(X_train, y_tr_onehot, lr=0.1, epochs=5000)\n",
    "y_pred = predict(X_test, best_W, best_b)\n",
    "print(\"准确率:\", accuracy(y_test, y_pred))"
   ],
   "id": "a1fd4bd8fba810d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率: 0.9666666666666667\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T17:20:19.171799Z",
     "start_time": "2025-08-10T17:20:19.170326Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "d10ff2cefc4bd892",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
